{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to obtain the unique headers amongst all the csv files that we are going to merge. Then we will finally make a master comma seperated value file with list of headers that we obtain from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parent directory and sub directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"../UnifiedCSVTestData/\"\n",
    "filelist = []\n",
    "dirs = []\n",
    "\n",
    "def makefilelist(parent_dir):\n",
    "    headers = []\n",
    "    csv_headers = []\n",
    "    subject_dirs = [os.path.join(parent_dir, dir) for dir in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, dir))]\n",
    "    filelist = []\n",
    "    for dir in subject_dirs:\n",
    "        csv_files = [os.path.join(dir, csv) for csv in os.listdir(dir) if os.path.isfile(os.path.join(dir, csv)) and csv.endswith('.csv')]\n",
    "        for file in csv_files:\n",
    "            filelist.append(file)\n",
    "    \n",
    "    return filelist, subject_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read headers from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(fileList, subject_dirs):\n",
    "    master_csv_headers = []\n",
    "    for filename in fileList:\n",
    "        slash = filename.split(\"/\")\n",
    "        parts = slash[2]\n",
    "        subparts = parts.split(\"_\")\n",
    "        CIK = subparts[0]\n",
    "        report_type = subparts[1]\n",
    "        subsubpart = subparts[2].split('-')\n",
    "        report_year = subsubpart[1]    \n",
    "        df = pd.read_csv(filename,engine='python')\n",
    "        df['CIK'] = CIK\n",
    "        df['Reporting Type'] = report_type\n",
    "        df['Report Year'] = report_year\n",
    "        df.to_csv(filename, encoding='utf-8', index=False)\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            d_reader = csv.DictReader(f)\n",
    "            headers = d_reader.fieldnames\n",
    "            for header in headers:\n",
    "                master_csv_headers.append(header)\n",
    "            \n",
    "    return master_csv_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist, dirs = makefilelist(parent_dir)\n",
    "csv_headers = readCSV(filelist, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will only use the unqiue headers that we might require for our master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueHeaders(csv_headers):\n",
    "    return set(csv_headers)\n",
    "\n",
    "def chomp(list1):\n",
    "    list1 = [x.replace('\\n', '') for x in list1]\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our unique header values and taking a look on the kind of values we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837\n",
      "1837\n"
     ]
    }
   ],
   "source": [
    "final_headers = uniqueHeaders(csv_headers)\n",
    "print(len(uniqueHeaders(csv_headers)))\n",
    "lol = chomp(final_headers)\n",
    "print(len(lol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for different instances of similar headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to test out the best conditions required to extract all the possible variants of each header in the unified CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy/Sell Protection1\n",
      "Buy/SellProtection (11)\n",
      "Buy/ Sell(1)\n",
      "Buy/Sell   Protection\n",
      "Protection \n",
      "Buy/SellProtection+\n",
      "Buy/SellProtection\n",
      "Buy/Sell Protection \n",
      "Buy/Sell\n",
      "Buy/Sell\n",
      "Protection\n",
      "BUY/SELLPROTECTION\n",
      "Buy/Sell  Protection\n",
      "Receive Buy/Sell Protection\n",
      "SELL PROTECTION  \n",
      "Protection (10)\n",
      "Buy/Sell   Protection \n",
      "Protection (9)\n",
      "Protection (1)\n",
      "Buy/Sell Protection\n",
      "Buy/SellProtection (15)\n",
      "BUY/SELL  PROTECTION\n",
      "Buy/Sell\n",
      "AnnualProtectionPayments\n",
      "Buy/Sell(1) \n",
      "Buy/SellProtection (1) \n",
      "BUY/SELL PROTECTION \n",
      "Protection (2)\n",
      "Buy/Sell Credit Protection\n",
      "Purchase/Sell Protection2\n",
      "Sell  Protection\n",
      "BUY/SELL PROTECTION\n",
      "Buy/Sell\n",
      "Protection \n",
      "Buy/sell protection\n",
      "Buy/Sell Protection1\n",
      "Buy/Sell Protection(2)\n",
      "Sell  Protection\n",
      "Buy/Sell Protection\n",
      "Buy/Sell Protection\n",
      "Buy/SellProtection\n",
      " Buy/SellProtection (8)\n",
      "Buy/Sell Protection\n",
      "Buy/Sell Protection\n",
      "Protection(1)\n",
      "Buy/Sell Protection (9)\n",
      "Buy/SellProtection (12)\n",
      "Buy/Sell\n",
      "Receive   Buy/Sell   Protection \n",
      "Buy/Sell Protection (1)\n",
      "BUY/SELL  PROTECTION \n",
      "BUY/SELLPROTECTION\n",
      "Buy/Sell Protection+\n",
      "Buy / Sell Protection\n",
      "Protection (1)\n",
      "BUY/SELL  PROTECTION\n",
      "Buy/Sell  Protection (2)\n",
      "Protection\n",
      "Buy/SellCredit Protection\n",
      "Buy/SaleProtection\n",
      "ReceiveBuy/SellProtection\n",
      "Buy/SellProtection (1)\n",
      "BUY/SELL PROTECTION\n",
      "Buy/Sell   Protection\n",
      "Buy/Sell \n",
      "BUY/SELL  PROTECTION \n",
      "  BUY/SELL  PROTECTION  \n",
      "Buy/SellProtection\n",
      "Buy/Sell Protection (11\n",
      "Protection2 \n",
      "Buy/SellProtection (7)\n",
      "Buy/\n",
      "ReceiveBuy/SellProtection\n",
      "Buy/Sell  Protection\n",
      "Receive   Buy/Sell   Protection\n",
      "Buy/Sell Protection1\n",
      "Buy/SellProtection\n",
      "Buy/Sell Protection (1)\n",
      "Buy/Sell    Protection\n",
      "Credit Protection\n",
      "PROTECTION\n",
      "Buy/Sell   Protection\n",
      "(Buy)/Sell Protection\n",
      "Buy/Sell  Protection (1)\n",
      "BUY/SELL   PROTECTION\n",
      "Buy/Sell Protection \n",
      "Buy/Sell Protection (2)\n",
      "Buy/Sell \n",
      "BUY/SELL PROTECTION\n",
      "Protection (7)\n",
      "BUY/SELL PROTECTION\n",
      "Receive  Buy/Sell  Protection\n",
      " Buy/Sell\n",
      "Buy/SellProtection1\n",
      "Buy/ Sell   Protection\n",
      "  Buy/Sell  Protection\n",
      "BUY/SELL   PROTECTION \n",
      "SELL  PROTECTION \n",
      "Buy/Sell  Protection (1)\n",
      "Buy/SellProtection\n",
      "BUY/SELL\n",
      "Buy/SellProtection (8)\n",
      "Buy/SellProtection (9)\n",
      "Buy/ Sell   Protection\n",
      "BUY  PROTECTION (1)\n",
      "Buy/Sell\n",
      "Buy/Sell Protection  \n",
      "PROTECTION\n",
      "Buy/Sell Protection (7)\n",
      "      Annual Protection      Payments\n",
      "Counter 110\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for header in lol:\n",
    "    if 'buy' in header.lower() or  ('sell' in header.lower() or 'protection' in header.lower()):\n",
    "        print(header)\n",
    "        counter+=1\n",
    "print (\"Counter\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will usse the above function as a way to guage how many similar ways are there to report a specific header and then merge all the occurances together to the master csv when we are conducting the joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the values present in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeValues(filelist):\n",
    "    for filename in filelist:\n",
    "        with open(filename, 'r') as f:\n",
    "            df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "            headers = list(df)\n",
    "            for header in headers:\n",
    "                if '000' in header:\n",
    "                    df[header] = df[header].astype(str) + '000'\n",
    "            print (df)\n",
    "            print(\"Filename\", filename)        \n",
    "            df.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normalizeValues(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate conditions for each type of Header "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will decide the final size of the unified CSV that we are going to generate for each time user searches for something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = [\"CIK\", \"Reporting Type\", \"Reporting Year\", \"Counterparty\", \"Notional Amount\", \"Reference Entity/Obligation\", \"Fixed Rate\", \"Expiration Date\", \"Appreciation/Depreciation\", \"Upfront Payments Paid/Received\", \"Implied Credit Spread\", \"Buy/Sell Protection\", \"Description\"]\n",
    "lol = pd.DataFrame()\n",
    "CIK_csv = pd.DataFrame(columns=[\"CIK\"])\n",
    "Reporting_Type_csv = pd.DataFrame(columns=[\"Reporting Type\"])\n",
    "Reporting_Year_csv = pd.DataFrame(columns=[\"Reporting Year\"])\n",
    "Counterparty_csv = pd.DataFrame(columns=[\"Counterparty\"])\n",
    "Notional_Amount_csv = pd.DataFrame(columns=[\"Notional Amount\"])\n",
    "Reference_Entity_csv = pd.DataFrame(columns=[\"Reference Entity/Obligation\"])\n",
    "Fixed_Rate_csv = pd.DataFrame(columns=[\"Fixed Rate\"])\n",
    "Expiration_Date_csv = pd.DataFrame(columns=[\"Expiration Date\"])\n",
    "Appreciation_csv = pd.DataFrame(columns=[\"Appreciation/Depreciation\"])\n",
    "Upfront_Payments_csv = pd.DataFrame(columns=[\"Upfront Payments Paid/Received\"])\n",
    "Buy_Sell_csv = pd.DataFrame(columns=[\"Buy/Sell Protection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the headers and merging them to a final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in filelist:\n",
    "    df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df = df.replace(r'\\t',' ', regex=True) \n",
    "    headers = list(df)\n",
    "    for header in headers:\n",
    "        #CIK NUMBER\n",
    "        if 'cik' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                CIK_csv = CIK_csv.append({'CIK': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'reporting' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Type_csv = Reporting_Type_csv.append({'Reporting Type': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'year' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Year_csv = Reporting_Year_csv.append({'Reporting Year': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'counter' in header.lower() or 'party' in header.lower() or 'obligation' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Counterparty_csv = Counterparty_csv.append({'Counterparty': i[1]}, ignore_index=True)\n",
    "        \n",
    "                \n",
    "        if 'notional' in header.lower() or 'amount' in header.lower() or 'value' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Notional_Amount_csv = Notional_Amount_csv.append({'Notional Amount': i[1]}, ignore_index=True)\n",
    "                               \n",
    "        if 'reference' in header.lower() or ('entity' in header.lower() or 'obligation' in header.lower()):\n",
    "            for i in df[header].iteritems():\n",
    "                Reference_Entity_csv = Reference_Entity_csv.append({'Reference Entity/Obligation': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'fixed' in header.lower() or 'rate' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Reference_Entity_csv = Reference_Entity_csv.append({'Fixed Rate': i[1]}, ignore_index=True)\n",
    "                \n",
    "        if 'date' in header.lower() or 'expiration' in header.lower() or 'termination' in header.lower() or 'maturity' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Expiration_Date_csv = Expiration_Date_csv.append({'Expiration Date': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'appreciation' in header.lower() or 'depriciation' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Appreciation_csv = Appreciation_csv.append({'Appreciation/Depreciation': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'upfront' in header.lower() or  ('payment' in header.lower() or 'premium' in header.lower()):\n",
    "            for i in df[header].iteritems():\n",
    "                Upfront_Payments_csv = Upfront_Payments_csv.append({'Upfront Payments Paid/Received': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'buy' in header.lower() or  ('sell' in header.lower() or 'protection' in header.lower()):\n",
    "            for i in df[header].iteritems():\n",
    "                Buy_Sell_csv = Buy_Sell_csv.append({'Buy/Sell Protection': i[1]}, ignore_index=True)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buy_Sell_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lol = CIK_csv.join(Reporting_Type_csv)\n",
    "lol = lol.join(Reporting_Year_csv)\n",
    "lol = lol.join(Counterparty_csv)\n",
    "lol = lol.join(Notional_Amount_csv)\n",
    "lol = lol.join(Reference_Entity_csv)\n",
    "lol = lol.join(Expiration_Date_csv)\n",
    "lol = lol.join(Appreciation_csv)\n",
    "lol = lol.join(Upfront_Payments_csv)\n",
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
