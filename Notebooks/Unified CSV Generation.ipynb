{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to obtain the unique headers amongst all the csv files that we are going to merge. Then we will finally make a master comma seperated value file with list of headers that we obtain from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parent directory and sub directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"../UnifiedCSVTestData/\"\n",
    "filelist = []\n",
    "dirs = []\n",
    "\n",
    "def makefilelist(parent_dir):\n",
    "    headers = []\n",
    "    csv_headers = []\n",
    "    subject_dirs = [os.path.join(parent_dir, dir) for dir in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, dir))]\n",
    "    filelist = []\n",
    "    for dir in subject_dirs:\n",
    "        csv_files = [os.path.join(dir, csv) for csv in os.listdir(dir) if os.path.isfile(os.path.join(dir, csv)) and csv.endswith('.csv')]\n",
    "        for file in csv_files:\n",
    "            filelist.append(file)\n",
    "    \n",
    "    return filelist, subject_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read headers from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(fileList, subject_dirs):\n",
    "    master_csv_headers = []\n",
    "    for filename in fileList:\n",
    "        slash = filename.split(\"/\")\n",
    "        parts = slash[2]\n",
    "        subparts = parts.split(\"_\")\n",
    "        CIK = subparts[0]\n",
    "        report_type = subparts[1]\n",
    "        subsubpart = subparts[2].split('-')\n",
    "        report_year = subsubpart[1]    \n",
    "        df = pd.read_csv(filename,engine='python')\n",
    "        df['CIK'] = CIK\n",
    "        df['Reporting Type'] = report_type\n",
    "        df['Report Year'] = report_year\n",
    "        df.to_csv(filename, encoding='utf-8', index=False)\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            d_reader = csv.DictReader(f)\n",
    "            headers = d_reader.fieldnames\n",
    "            for header in headers:\n",
    "                master_csv_headers.append(header)\n",
    "            \n",
    "    return master_csv_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist, dirs = makefilelist(parent_dir)\n",
    "csv_headers = readCSV(filelist, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will only use the unqiue headers that we might require for our master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueHeaders(csv_headers):\n",
    "    return set(csv_headers)\n",
    "\n",
    "def chomp(list1):\n",
    "    list1 = [x.replace('\\n', '') for x in list1]\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our unique header values and taking a look on the kind of values we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837\n",
      "1837\n"
     ]
    }
   ],
   "source": [
    "final_headers = uniqueHeaders(csv_headers)\n",
    "print(len(uniqueHeaders(csv_headers)))\n",
    "lol = chomp(final_headers)\n",
    "print(len(lol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for different instances of similar headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to test out the best conditions required to extract all the possible variants of each header in the unified CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Type\n",
      "Counter 1\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for header in lol:\n",
    "    if 'reporting' in header.lower():\n",
    "        print(header)\n",
    "        counter+=1\n",
    "print (\"Counter\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will usse the above function as a way to guage how many similar ways are there to report a specific header and then merge all the occurances together to the master csv when we are conducting the joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the values present in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeValues(filelist):\n",
    "    for filename in filelist:\n",
    "        with open(filename, 'r') as f:\n",
    "            df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "            headers = list(df)\n",
    "            for header in headers:\n",
    "                if '000' in header:\n",
    "                    df[header] = df[header].astype(str) + '000'\n",
    "            print (df)\n",
    "            print(\"Filename\", filename)        \n",
    "            df.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normalizeValues(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate conditions for each type of Header "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will decide the final size of the unified CSV that we are going to generate for each time user searches for something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = [\"CIK\", \"Reporting Type\", \"Reporting Year\", \"Counterparty\", \"Notional Amount\", \"Reference Entity/Obligation\", \"Fixed Rate\", \"Expiration Date\", \"Appreciation/Depreciation\", \"Upfront Payments Paid/Received\", \"Implied Credit Spread\", \"Buy/Sell Protection\", \"Description\"]\n",
    "CIK_csv = pd.DataFrame(columns=[\"CIK\"])\n",
    "Reporting_Type_csv = pd.DataFrame(columns=[\"Reporting Type\"])\n",
    "Notional_Amount_csv = pd.DataFrame(columns=[\"Notional Amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIK Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in filelist:\n",
    "    df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df = df.replace(r'\\t',' ', regex=True) \n",
    "    headers = list(df)\n",
    "    for header in headers:\n",
    "        #CIK NUMBER\n",
    "        if 'cik' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                CIK_csv = CIK_csv.append({'CIK': i[1]}, ignore_index=True)\n",
    "        \n",
    "        if 'reporting' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Type_csv = Reporting_Type_csv.append({'Reporting Type': i[1]}, ignore_index=True)\n",
    "                \n",
    "        if 'notional' in header.lower() or 'amount' in header.lower() or 'value' in header.lower():\n",
    "            for i in df[header].iteritems():\n",
    "                Notional_Amount_csv = Notional_Amount_csv.append({'Notional Amount': i[1]}, ignore_index=True)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reporting_Type_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
