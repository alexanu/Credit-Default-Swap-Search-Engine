{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to obtain the unique headers amongst all the csv files that we are going to merge. Then we will finally make a master comma seperated value file with list of headers that we obtain from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parent directory and sub directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"../UnifiedCSVTestData/\"\n",
    "filelist = []\n",
    "dirs = []\n",
    "\n",
    "def makefilelist(parent_dir):\n",
    "    headers = []\n",
    "    csv_headers = []\n",
    "    subject_dirs = [os.path.join(parent_dir, dir) for dir in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, dir))]\n",
    "    filelist = []\n",
    "    for dir in subject_dirs:\n",
    "        csv_files = [os.path.join(dir, csv) for csv in os.listdir(dir) if os.path.isfile(os.path.join(dir, csv)) and csv.endswith('.csv')]\n",
    "        for file in csv_files:\n",
    "            filelist.append(file)\n",
    "    \n",
    "    return filelist, subject_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read headers from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(fileList, subject_dirs):\n",
    "    master_csv_headers = []\n",
    "    for filename in fileList:\n",
    "        slash = filename.split(\"/\")\n",
    "        parts = slash[2]\n",
    "        subparts = parts.split(\"_\")\n",
    "        CIK = subparts[0]\n",
    "        report_type = subparts[1]\n",
    "        subsubpart = subparts[2].split('-')\n",
    "        report_year = subsubpart[1]    \n",
    "        df = pd.read_csv(filename,engine='python')\n",
    "        df['CIK'] = CIK\n",
    "        df['Reporting Type'] = report_type\n",
    "        df['Report Year'] = report_year\n",
    "        df.to_csv(filename, encoding='utf-8', index=False)\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            d_reader = csv.DictReader(f)\n",
    "            headers = d_reader.fieldnames\n",
    "            for header in headers:\n",
    "                master_csv_headers.append(header)\n",
    "            \n",
    "    return master_csv_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist, dirs = makefilelist(parent_dir)\n",
    "csv_headers = readCSV(filelist, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will only use the unqiue headers that we might require for our master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueHeaders(csv_headers):\n",
    "    return set(csv_headers)\n",
    "\n",
    "def chomp(list1):\n",
    "    list1 = [x.replace('\\n', '') for x in list1]\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our unique header values and taking a look on the kind of values we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837\n",
      "1837\n"
     ]
    }
   ],
   "source": [
    "final_headers = uniqueHeaders(csv_headers)\n",
    "print(len(uniqueHeaders(csv_headers)))\n",
    "lol = chomp(final_headers)\n",
    "print(len(lol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for different instances of similar headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to test out the best conditions required to extract all the possible variants of each header in the unified CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter 0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for header in lol:\n",
    "    if 'des' in lol:\n",
    "        print(header)\n",
    "        counter+=1\n",
    "print (\"Counter\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will usse the above function as a way to guage how many similar ways are there to report a specific header and then merge all the occurances together to the master csv when we are conducting the joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the values present in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeValues(filelist):\n",
    "    for filename in filelist:\n",
    "        with open(filename, 'r') as f:\n",
    "            df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "            headers = list(df)\n",
    "            for header in headers:\n",
    "                if '000' in header:\n",
    "                    df[header] = df[header].astype(str) + '000'\n",
    "            print (df)\n",
    "            print(\"Filename\", filename)        \n",
    "            df.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normalizeValues(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate conditions for each type of Header "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will decide the final size of the unified CSV that we are going to generate for each time user searches for something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = [\"CIK\", \"Reporting Type\", \"Reporting Year\", \"Counterparty\", \"Notional Amount\", \"Reference Entity/Obligation\", \"Fixed Rate\", \"Expiration Date\", \"Appreciation/Depreciation\", \"Upfront Payments Paid/Received\", \"Implied Credit Spread\", \"Buy/Sell Protection\", \"Description\"]\n",
    "unified_csv = pd.DataFrame()\n",
    "CIK_csv = pd.DataFrame(columns=[\"CIK\"])\n",
    "Reporting_Type_csv = pd.DataFrame(columns=[\"Reporting Type\"])\n",
    "Reporting_Year_csv = pd.DataFrame(columns=[\"Reporting Year\"])\n",
    "Counterparty_csv = pd.DataFrame(columns=[\"Counterparty\"])\n",
    "Notional_Amount_csv = pd.DataFrame(columns=[\"Notional Amount\"])\n",
    "Reference_Entity_csv = pd.DataFrame(columns=[\"Reference Entity/Obligation\"])\n",
    "Fixed_Rate_csv = pd.DataFrame(columns=[\"Fixed Rate\"])\n",
    "Expiration_Date_csv = pd.DataFrame(columns=[\"Expiration Date\"])\n",
    "Appreciation_csv = pd.DataFrame(columns=[\"Appreciation/Depreciation\"])\n",
    "Upfront_Payments_csv = pd.DataFrame(columns=[\"Upfront Payments Paid/Received\"])\n",
    "Buy_Sell_csv = pd.DataFrame(columns=[\"Buy/Sell Protection\"])\n",
    "Description_csv = pd.DataFrame(columns=[\"Description\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the headers and merging them to a final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2fc3edd6c0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mBuy_Sell_Flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mBuy_Sell_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuy_Sell_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Buy/Sell Protection'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mBuy_Sell_Flag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   4526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4527\u001b[0m             combined_columns = self.columns.tolist() + self.columns.union(\n\u001b[0;32m-> 4528\u001b[0;31m                 other.index).difference(self.columns).tolist()\n\u001b[0m\u001b[1;32m   4529\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4530\u001b[0m             other = DataFrame(other.values.reshape((1, len(other))),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;31m# for subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_union_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_union_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_wrap_union_result\u001b[0;34m(self, other, result)\u001b[0m\n\u001b[1;32m   2208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_union_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# _asarray_tuplesafe does not always copy underlying data,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36m_asarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for filename in filelist:\n",
    "    #Add flgas to detect changes\n",
    "    CIK_Flag = 0\n",
    "    ReportingType_Flag = 0\n",
    "    ReportingYear_Flag = 0\n",
    "    Counterparty_Flag = 0\n",
    "    NotionalAmount_Flag = 0\n",
    "    ReferenceEntity_Flag = 0\n",
    "    FixedRate_Flag = 0\n",
    "    ExpirationDate_Flag = 0\n",
    "    App_Dep_Flag = 0\n",
    "    Upfront_Payments_Flag = 0\n",
    "    Buy_Sell_Flag = 0\n",
    "    Description_Flag = 0\n",
    "    \n",
    "    #Reading and performing basic data cleaning \n",
    "    df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df = df.replace(r'\\t',' ', regex=True)\n",
    "    #Putting the list of headers in a list\n",
    "    headers = list(df)\n",
    "    \n",
    "    #Checking for CIK in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        #CIK NUMBER\n",
    "        if 'cik' in header.lower():\n",
    "            CIK_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                CIK_csv = CIK_csv.append({'CIK': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if CIK_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            CIK_csv = CIK_csv.append({'CIK': \"NaN\"}, ignore_index=True)\n",
    "            \n",
    "    #Checking for Reporting Type in the list if not append NaN for all rows        \n",
    "    for header in headers:    \n",
    "        if 'reporting' in header.lower():\n",
    "            ReportingType_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Type_csv = Reporting_Type_csv.append({'Reporting Type': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if ReportingType_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Type_csv = Reporting_Type_csv.append({'Reporting Type': \"NaN\"}, ignore_index=True)\n",
    "            \n",
    "    #Checking for Reporting Year in the list if not append NaN for all rows   \n",
    "    for header in headers:\n",
    "        if 'year' in header.lower():\n",
    "            ReportingYear_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Year_csv = Reporting_Year_csv.append({'Reporting Year': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if ReportingYear_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Year_csv = Reporting_Year_csv.append({'Reporting Year': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Counterparty in the list if not append NaN for all rows\n",
    "    for header in headers:    \n",
    "        if 'counter' in header.lower() or 'party' in header.lower() or 'obligation' in header.lower():\n",
    "            Counterparty_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Counterparty_csv = Counterparty_csv.append({'Counterparty': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if Counterparty_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Year_csv = Reporting_Year_csv.append({'Counterparty': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Notional Amount in the list if not append NaN for all rows\n",
    "    for header in headers:           \n",
    "        if 'notional' in header.lower() or 'amount' in header.lower() or 'value' in header.lower():\n",
    "            NotionalAmount_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Notional_Amount_csv = Notional_Amount_csv.append({'Notional Amount': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if NotionalAmount_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Year_csv = Reporting_Year_csv.append({'Notional Amount': \"NaN\"}, ignore_index=True)\n",
    "                               \n",
    "    #Checking for Reference Entity/Obligation in the list if not append NaN for all rows             \n",
    "    for header in headers:              \n",
    "        if 'reference' in header.lower() or ('entity' in header.lower() or 'obligation' in header.lower()):\n",
    "            ReferenceEntity_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                \n",
    "                Reference_Entity_csv = Reference_Entity_csv.append({'Reference Entity/Obligation': i[1]}, ignore_index=True)\n",
    "    if ReferenceEntity_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Year_csv = Reporting_Year_csv.append({'Reference Entity/Obligation': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Fixed Rate in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'fixed' in header.lower() or 'rate' in header.lower():\n",
    "            FixedRate_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Reference_Entity_csv = Reference_Entity_csv.append({'Fixed Rate': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if FixedRate_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Year_csv = Reporting_Year_csv.append({'Fixed Rate': \"NaN\"}, ignore_index=True)  \n",
    "            \n",
    "     #Checking for Expiration Date in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'date' in header.lower() or 'expiration' in header.lower() or 'termination' in header.lower() or 'maturity' in header.lower():\n",
    "            ExpirationDate_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Expiration_Date_csv = Expiration_Date_csv.append({'Expiration Date': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if ExpirationDate_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Expiration_Date_csv = Expiration_Date_csv.append({'Expiration Date': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Appreciation Depreciation in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'appreciation' in header.lower() or 'depriciation' in header.lower():\n",
    "            App_Dep_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Appreciation_csv = Appreciation_csv.append({'Appreciation/Depreciation': i[1]}, ignore_index=True)\n",
    "    \n",
    "    if App_Dep_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Appreciation_csv = Appreciation_csv.append({'Appreciation/Depreciation': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Upfront Payments Paid/Received in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'upfront' in header.lower() or  ('payment' in header.lower() or 'premium' in header.lower()):\n",
    "            Upfront_Payments_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Upfront_Payments_csv = Upfront_Payments_csv.append({'Upfront Payments Paid/Received': i[1]}, ignore_index=True)\n",
    "    \n",
    "    if Upfront_Payments_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Upfront_Payments_csv = Upfront_Payments_csv.append({'Upfront Payments Paid/Received': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    #Checking for Buy/Sell Protection in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'buy' in header.lower() or  'sell' in header.lower() or 'protection' in header.lower():\n",
    "            Buy_Sell_Flag = 0\n",
    "            for i in df[header].iteritems():\n",
    "                Buy_Sell_csv = Buy_Sell_csv.append({'Buy/Sell Protection': i[1]}, ignore_index=True)\n",
    "    if Buy_Sell_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Buy_Sell_csv = Upfront_Payments_csv.append({'Buy/Sell Protection': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Description in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'description' in header.lower():\n",
    "            Description_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Description_csv = Description_csv.append({'Description': i[1]}, ignore_index=True)\n",
    "    if Description_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Description_csv = Description_csv.append({'Description': \"NaN\"}, ignore_index=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Description_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_csv = CIK_csv.join(Reporting_Type_csv)\n",
    "unified_csv = unified_csv.join(Reporting_Year_csv)\n",
    "unified_csv = unified_csv.join(Counterparty_csv)\n",
    "unified_csv = unified_csv.join(Notional_Amount_csv)\n",
    "unified_csv = unified_csv.join(Reference_Entity_csv)\n",
    "unified_csv = unified_csv.join(Expiration_Date_csv)\n",
    "unified_csv = unified_csv.join(Appreciation_csv)\n",
    "unified_csv = unified_csv.join(Upfront_Payments_csv)\n",
    "unified_csv = unified_csv.join(Buy_Sell_csv)\n",
    "unified_csv = unified_csv.join(Description_csv)\n",
    "unified_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
